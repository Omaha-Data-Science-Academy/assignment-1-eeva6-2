{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d37a6bc0",
      "metadata": {
        "id": "d37a6bc0"
      },
      "source": [
        "# Assignment 1 - Binary Classification Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1679a82",
      "metadata": {
        "id": "d1679a82"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this assignment is to assess your understanding of fundamental concepts in model evaluation for machine learning tasks. This assignment covers topics discussed in the first half of the course, including key evaluation metrics, confusion matrices, ROC curves, and Precision-Recall curves.\n",
        "Instructions:\n",
        "\n",
        "1. Theory Questions:\n",
        "Answer the following theoretical questions:\n",
        "\n",
        "    1. Explain the limitations of accuracy as an evaluation metric in imbalanced datasets. How does accuracy behave when classes are heavily skewed, and why might it provide misleading results?\n",
        "    2. Describe the purpose and interpretation of a confusion matrix. How does it help in assessing a classification model's performance?\n",
        "    3. Explain the concept of ROC curves. What does each point on an ROC curve represent? How is the area under the ROC curve (AUC-ROC) calculated?\n",
        "    4. Compare and contrast the advantages and disadvantages of ROC curves and Precision-Recall curves. In what scenarios would you prefer to use one over the other, and why?\n",
        "\n",
        "2. Practical Exercises:\n",
        "* Implement Python code to calculate the following evaluation metrics for a given binary classification problem: Log Loss\n",
        "* Select the best metric for an applied scenario\n",
        "\n",
        "**Submission Guidelines:**\n",
        "* Submit your responses to the theory questions in a neatly organized markdown.\n",
        "* Include your Python code for the practical exercise.\n",
        "* Submit your assignment as a single `.ipynb` file named `MY NAME Assignment 1 - Log Loss` via the course submission platform (slack)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58864fe",
      "metadata": {
        "id": "e58864fe"
      },
      "source": [
        "## Part 1: Theory Questions (20 points)\n",
        "Provide your answers here:\n",
        "\n",
        "    1.Accuracy measures the percentage of correct predictions made by the model. However, in imbalanced datasets (where one class is much more common than the other), accuracy may be misleading. If, say, 95% of samples are type A and only 5% are type B, a model that always predicts type A qwill be 95% accurate, but is never actually detecting the minority class. Also accuracy doesn't tell us about false positives or false negatives.\n",
        "    2. A confusion matrix is a 2x2 table for binary classification problems. It shows how many predictions fall into each category; Predictive Positive or Negative, vs Actual Positive or Negative. It helps to visualize performance, and allows calculation of key metrics liek precision, recall, F!-score, and specificity.\n",
        "    3. Receiver Operating Characteristic plots the true positive rate (recall) vs False Positive Rate. Each point on the ROC curve shows the model's performance at a specific decision threshold. The AUC (area under the curve) measures teh area under the ROC curve, ranges from 0 to 1, and higher is better. AUC = .5 is the equivalent of random guessing, and AUC = 1 is perfect classification.\n",
        "    4. ROC curves work best for balanced datasets, but can be misleading when the dataset is imbalanced as even a high false positive rate may seem low due to many true negatives. Precidion-Recall Curves work best for imbalanced dataset, as they show how well the model detects positives without being misled by many negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b8c2adb",
      "metadata": {
        "id": "8b8c2adb"
      },
      "source": [
        "## Practicing Log Loss (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4dca41",
      "metadata": {
        "id": "2f4dca41"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this assignment is to deepen your understanding of log loss, also known as logarithmic loss or cross-entropy loss, and its application in evaluating the performance of classification models.\n",
        "\n",
        "**Instructions:**\n",
        "In this assignment, you will be given a set of binary classification predictions along with their corresponding actual class labels. Your task is to calculate the log loss for each prediction and then analyze the overall log loss performance of the model.\n",
        "\n",
        "**Dataset:**\n",
        "You are provided with a dataset containing the following information:\n",
        "\n",
        "Predicted probabilities for the positive class (ranging from 0 to 1) for a set of instances.\n",
        "Actual binary class labels (0 or 1) indicating whether the instance belongs to the positive class or not.\n",
        "\n",
        "**Assignment Tasks:**\n",
        "1. Calculate the log loss for each instance in the dataset using the predicted probabilities and actual class labels.\n",
        "2. Summarize the individual log losses and compute the overall log loss performance for the model.\n",
        "3. Interpret the overall log loss value and analyze the model's performance. Discuss any insights or observations derived from the log loss analysis.\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "| Instance | Predicted Probability | Actual Label |\n",
        "|----------|------------------------|--------------|\n",
        "|    1     |          0.9           |       1      |\n",
        "|    2     |          0.3           |       0      |\n",
        "|    3     |          0.6           |       1      |\n",
        "|    4     |          0.8           |       0      |\n",
        "|    5     |          0.1           |       1      |\n",
        "\n",
        "\n",
        "**Grading Criteria:**\n",
        "\n",
        "* Correctness of log loss calculations.\n",
        "* Clarity and completeness of the analysis.\n",
        "* Insights derived from the log loss interpretation.\n",
        "* Overall presentation and adherence to submission guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3dad6832",
      "metadata": {
        "id": "3dad6832",
        "outputId": "e9d50441-9df8-4705-a971-a195ad9e599a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Instance  Predicted Probability  Actual Label\n",
            "0         1                    0.9             1\n",
            "1         2                    0.3             0\n",
            "2         3                    0.6             1\n",
            "3         4                    0.8             0\n",
            "4         5                    0.1             1\n",
            "\n",
            "Data with Log Loss:\n",
            "   Instance  Predicted Probability  Actual Label  Log Loss\n",
            "0         1                    0.9             1  0.105361\n",
            "1         2                    0.3             0  0.356675\n",
            "2         3                    0.6             1  0.510826\n",
            "3         4                    0.8             0  1.609438\n",
            "4         5                    0.1             1  2.302585\n",
            "\n",
            "Overall Log Loss: 0.9770\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame with the dataset\n",
        "data = {\n",
        "    'Instance': [1, 2, 3, 4, 5],\n",
        "    'Predicted Probability': [0.9, 0.3, 0.6, 0.8, 0.1],\n",
        "    'Actual Label': [1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "\n",
        "# --- Add this part below ---\n",
        "# Define a function to compute log loss for one prediction\n",
        "def log_loss(p, y):\n",
        "    p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "    return -(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "\n",
        "# Apply the log loss function to each row\n",
        "df['Log Loss'] = df.apply(lambda row: log_loss(row['Predicted Probability'], row['Actual Label']), axis=1)\n",
        "\n",
        "# Calculate overall log loss\n",
        "overall_log_loss = df['Log Loss'].mean()\n",
        "\n",
        "# Display the updated DataFrame with Log Loss\n",
        "print(\"\\nData with Log Loss:\")\n",
        "print(df)\n",
        "\n",
        "# Print the overall Log Loss\n",
        "print(f\"\\nOverall Log Loss: {overall_log_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9be2a3c3",
      "metadata": {
        "id": "9be2a3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b13d2d-9ca0-482f-d9bd-28bba2956ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Instance  Predicted Probability  Actual Label\n",
            "0         1                    0.6             1\n",
            "1         2                    0.3             0\n",
            "2         3                    0.6             1\n",
            "3         4                    0.8             0\n",
            "4         5                    0.1             1\n",
            "\n",
            "Data with Log Loss:\n",
            "   Instance  Predicted Probability  Actual Label  Log Loss\n",
            "0         1                    0.6             1  0.510826\n",
            "1         2                    0.3             0  0.356675\n",
            "2         3                    0.6             1  0.510826\n",
            "3         4                    0.8             0  1.609438\n",
            "4         5                    0.1             1  2.302585\n",
            "\n",
            "Overall Log Loss: 1.0581\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame with the dataset\n",
        "data = {\n",
        "    'Instance': [1, 2, 3, 4, 5],\n",
        "    'Predicted Probability': [0.6, 0.3, 0.6, 0.8, 0.1],\n",
        "    'Actual Label': [1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "\n",
        "# --- Add this part below ---\n",
        "# Define a function to compute log loss for one prediction\n",
        "def log_loss(p, y):\n",
        "    p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "    return -(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "\n",
        "# Apply the log loss function to each row\n",
        "df['Log Loss'] = df.apply(lambda row: log_loss(row['Predicted Probability'], row['Actual Label']), axis=1)\n",
        "\n",
        "# Calculate overall log loss\n",
        "overall_log_loss = df['Log Loss'].mean()\n",
        "\n",
        "# Display the updated DataFrame with Log Loss\n",
        "print(\"\\nData with Log Loss:\")\n",
        "print(df)\n",
        "\n",
        "# Print the overall Log Loss\n",
        "print(f\"\\nOverall Log Loss: {overall_log_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa92db1",
      "metadata": {
        "id": "eaa92db1"
      },
      "source": [
        "*Question: Interpret the log loss above. How would it change if the predicted probability for instance 0 changed from 0.9 to 0.6? Why?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2abec414",
      "metadata": {
        "id": "2abec414"
      },
      "source": [
        " Log loss penalizes incorrect and overconfident predictions.The Original Predicted Probability of 0.9 is close to the truth, so low log loss. The New Predicted Probability: 0.6 is less confident about a correct label. Old log loss: .105, New Log loss for .6 prediction: .511, so the log loss increases with the lower predicted probability. This means the model is now less confident about a corredc prediction, so the log loss increases. It also increases the overall average log loss, signaling worse model performance.\n",
        " Log loss rewards correct, confident predictions and penalizes wrong or unsure ones, especially when they're confidently wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "972c7485",
      "metadata": {
        "id": "972c7485"
      },
      "source": [
        "*Question: Why might you select log loss over precision, recall, or accuracy (in the context of any problem, not this one specifically)?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b26ee2",
      "metadata": {
        "id": "88b26ee2"
      },
      "source": [
        "You'd choose log loss when you care about how confident your predictions are. A model that predicts .99 when it's correct gets rewarded, but if it predicts .99 and is wrong, it's penalized. This is especailly useful in hight stakes applicaitons like medical diagnosis or fraud detection, where it's vital to know how confident the modle is in it's prediction. It's also useful for comoparing probabilistic models like logistic regression, XGBoost, neural networks, etc. and helps to distinguish between two models that both have the right answer but with different confidence. re: the other metrics, Accuracy tells you the % of correct predictions, but is misleading on imbalanced data. Precision  = % of predicted positives that are correct, but ignores false negatives. Recal = % of actual positives that are correclty predicted, but ignores false positives. Log loss measures the quality of probability estimates. So, to summarize, we use log loss when: we are ranking models that output probabilities, when the cost of a confident wrong prediction is high, and when we are optimizing for overall predictive uncerqainty rather than individual yes/no outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69ed6d7",
      "metadata": {
        "id": "a69ed6d7"
      },
      "source": [
        "## Application Scenario: Select a Metric (55 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791158c2",
      "metadata": {
        "id": "791158c2"
      },
      "source": [
        "**Application Scenario: Fraud Detection System**\n",
        "\n",
        "You are working as a data scientist for a financial institution that wants to develop a fraud detection system to identify potentially fraudulent transactions. The dataset contains information about various transactions, including transaction amount, merchant ID, and transaction type. Your task is to build a machine learning model to classify transactions as either fraudulent or non-fraudulent.\n",
        "\n",
        "**Problem Description:**\n",
        "\n",
        "* Dataset: The dataset consists of historical transaction data, with labels indicating whether each transaction was fraudulent or not.\n",
        "* Class Distribution: The dataset is mostly non-fraudulant cases, with a small percentage of transactions being fraudulent compared to legitimate transactions.\n",
        "* Objective: The objective is to develop a fraud detection model that minimizes false negatives (fraudulent transactions incorrectly classified as non-fraudulent) while maintaining a reasonable level of precision.\n",
        "\n",
        "**Stakeholder Requirements:**\n",
        "Given the nature of the problem, it is crucial to prioritize recall (sensitivity) to ensure that as many fraudulent transactions as possible are detected. However, precision is also important to minimize false positives and avoid unnecessary investigations of legitimate transactions. Minimizing false negatives (missing fraudulent transactions) is of utmost importance.\n",
        "\n",
        "**Task:**\n",
        "Your task is to develop Python code to evaluate the performance of different machine learning models using various evaluation metrics, including accuracy, precision, recall, and F2 score. *Select the evaluation metric that best suits the problem and explain your choice*.\n",
        "\n",
        "**Additional Guidelines:**\n",
        "* You should preprocess the dataset as needed and split it into training and testing sets.\n",
        "* Implement machine learning models of your choice (e.g., logistic regression, random forest) and evaluate their performance.\n",
        "* Use appropriate evaluation metrics for binary classification tasks.\n",
        "* Discuss the rationale behind your choice of evaluation metric and how it aligns with the problem requirements.\n",
        "* Present your findings and recommendations for selecting the best model based on the chosen evaluation metric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4f0e46",
      "metadata": {
        "id": "dc4f0e46"
      },
      "source": [
        "**Dataset Sample:**\n",
        "\n",
        "| Transaction ID | Transaction Amount | Merchant ID | Transaction Type | Fraudulent |\n",
        "|----------------|--------------------|-------------|------------------|------------|\n",
        "| 1              | 1000               | M123        | Online Purchase  | 0          |\n",
        "| 2              | 500                | M456        | ATM Withdrawal   | 0          |\n",
        "| 3              | 2000               | M789        | Online Purchase  | 1          |\n",
        "| 4              | 1500               | M123        | POS Transaction  | 0          |\n",
        "| 5              | 800                | M456        | Online Purchase  | 0          |\n",
        "| 6              | 3000               | M789        | ATM Withdrawal   | 1          |\n",
        "\n",
        "* Transaction ID: Unique identifier for each transaction.\n",
        "* Transaction Amount: The amount of money involved in the transaction.\n",
        "* Merchant ID: Identifier for the merchant involved in the transaction.\n",
        "* Transaction Type: The type of transaction (e.g., online purchase, ATM withdrawal, POS transaction).\n",
        "* Fraudulent: Binary indicator (0 or 1) specifying whether the transaction is fraudulent (1) or not (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1bc1ec81",
      "metadata": {
        "id": "1bc1ec81",
        "outputId": "ee3d5bb6-2495-4c3a-a3f1-61577e761733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Transaction ID  Transaction Amount Merchant ID Transaction Type  \\\n",
            "0                1                1000        M123  Online Purchase   \n",
            "1                2                 500        M456   ATM Withdrawal   \n",
            "2                3                2000        M789  Online Purchase   \n",
            "3                4                1500        M123  POS Transaction   \n",
            "4                5                 800        M456  Online Purchase   \n",
            "5                6                3000        M789   ATM Withdrawal   \n",
            "6                7                1200        M123  Online Purchase   \n",
            "7                8                 700        M456   ATM Withdrawal   \n",
            "8                9                1800        M789  Online Purchase   \n",
            "9               10                1300        M123  POS Transaction   \n",
            "10              11                 900        M456  Online Purchase   \n",
            "11              12                 400        M789   ATM Withdrawal   \n",
            "12              13                2200        M123  Online Purchase   \n",
            "13              14                1600        M456   ATM Withdrawal   \n",
            "14              15                 850        M789  Online Purchase   \n",
            "15              16                2800        M123  POS Transaction   \n",
            "16              17                1100        M456  Online Purchase   \n",
            "17              18                 600        M789   ATM Withdrawal   \n",
            "18              19                1900        M123  Online Purchase   \n",
            "19              20                1400        M456   ATM Withdrawal   \n",
            "20              21                 950        M123  Online Purchase   \n",
            "21              22                 300        M456   ATM Withdrawal   \n",
            "22              23                2100        M789  Online Purchase   \n",
            "23              24                1700        M123  POS Transaction   \n",
            "24              25                 820        M456  Online Purchase   \n",
            "25              26                3200        M789   ATM Withdrawal   \n",
            "26              27                1250        M123  Online Purchase   \n",
            "27              28                 720        M456   ATM Withdrawal   \n",
            "28              29                1850        M789  Online Purchase   \n",
            "29              30                1350        M123  POS Transaction   \n",
            "30              31                 880        M456  Online Purchase   \n",
            "31              32                 420        M789   ATM Withdrawal   \n",
            "32              33                2400        M123  Online Purchase   \n",
            "33              34                1750        M456   ATM Withdrawal   \n",
            "34              35                 830        M789  Online Purchase   \n",
            "35              36                3100        M123  POS Transaction   \n",
            "36              37                1150        M456  Online Purchase   \n",
            "37              38                 620        M789   ATM Withdrawal   \n",
            "38              39                1950        M123  Online Purchase   \n",
            "39              40                1450        M456   ATM Withdrawal   \n",
            "\n",
            "    Fraudulent  \n",
            "0            0  \n",
            "1            0  \n",
            "2            1  \n",
            "3            0  \n",
            "4            0  \n",
            "5            1  \n",
            "6            0  \n",
            "7            0  \n",
            "8            1  \n",
            "9            0  \n",
            "10           0  \n",
            "11           1  \n",
            "12           0  \n",
            "13           0  \n",
            "14           1  \n",
            "15           0  \n",
            "16           0  \n",
            "17           1  \n",
            "18           0  \n",
            "19           0  \n",
            "20           1  \n",
            "21           0  \n",
            "22           0  \n",
            "23           1  \n",
            "24           0  \n",
            "25           0  \n",
            "26           1  \n",
            "27           0  \n",
            "28           0  \n",
            "29           1  \n",
            "30           0  \n",
            "31           0  \n",
            "32           1  \n",
            "33           0  \n",
            "34           0  \n",
            "35           1  \n",
            "36           0  \n",
            "37           0  \n",
            "38           1  \n",
            "39           0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating the dataset\n",
        "data = {\n",
        "    'Transaction ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
        "                       11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
        "                       21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
        "                       31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
        "    'Transaction Amount': [1000, 500, 2000, 1500, 800, 3000, 1200, 700, 1800, 1300,\n",
        "                           900, 400, 2200, 1600, 850, 2800, 1100, 600, 1900, 1400,\n",
        "                           950, 300, 2100, 1700, 820, 3200, 1250, 720, 1850, 1350,\n",
        "                           880, 420, 2400, 1750, 830, 3100, 1150, 620, 1950, 1450],\n",
        "    'Merchant ID': ['M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123',\n",
        "                    'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456',\n",
        "                    'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123',\n",
        "                    'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456'],\n",
        "    'Transaction Type': ['Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction', 'Online Purchase',\n",
        "                         'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase',\n",
        "                         'POS Transaction', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction', 'Online Purchase',\n",
        "                         'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase',\n",
        "                         'POS Transaction', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal'],\n",
        "    'Fraudulent': [0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
        "                   0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
        "                   1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
        "                   0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4eb158cb",
      "metadata": {
        "id": "4eb158cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ee1a60-ea4f-4467-965c-11b22d806753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Metrics:\n",
            "Accuracy:  0.50\n",
            "Precision: 0.25\n",
            "Recall:    0.33\n",
            "F2 Score:  0.31\n",
            "\n",
            "📊 Evaluation Metrics:\n",
            "Accuracy:  0.50\n",
            "Precision: 0.25\n",
            "Recall:    0.33\n",
            "F2 Score:  0.31\n",
            "\n",
            "📊 Evaluation Metrics:\n",
            "Accuracy:  0.50\n",
            "Precision: 0.25\n",
            "Recall:    0.33\n",
            "F2 Score:  0.31\n",
            "\n",
            "🧾 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.57      0.62         7\n",
            "           1       0.25      0.33      0.29         3\n",
            "\n",
            "    accuracy                           0.50        10\n",
            "   macro avg       0.46      0.45      0.45        10\n",
            "weighted avg       0.54      0.50      0.52        10\n",
            "\n",
            "In evaluating metrics for this solution, we need consider that a false negative (missing an actual fraudulent transaction) means that fraud goes undetected, which could result in financial losses and damage to the institutional reputation. A false positive (flagging a legit transaction as fraud) could inconvenience users and lead to unnecessary investigations, but is less costly than missing actual fraud. Therefore we should proceed with the goal of maximizing recall (sensitivity) so that we catch as many fraudulent transactions as possible, while maintaining a reasonable level of precision to avoid too many false alarms.  To balance this, I’d select the F2 score as a main. Evaluation metric, as it gives more weight to recall than precision, and aligns with the requirement to minimize FN while still considering FP.  Based on using a random forest classifier, chosen because it works well for imbalanced classification problems like fraud detection, handles both numerical and categorical data, is less sensitive to noise and overfitting compared to a single decision tree, and ranks feature importance, helping with interpretability.   Findings: based on results, the model achieves a moderate recall (.330, meaning it correctly identifies a fair number of fraud cases. Precision is lower, due to the class imbalance and nature of fraud detection. The F2 of .31 provides a holistic view of performance that prioritizes fraud detection coverage. \n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, classification_report\n",
        "\n",
        "# Step 1: Define features and target\n",
        "X = df.drop(columns=['Transaction ID', 'Fraudulent'])\n",
        "y = df['Fraudulent']\n",
        "\n",
        "# Step 2: Preprocessing (OneHotEncode categorical columns)\n",
        "categorical_features = ['Merchant ID', 'Transaction Type']\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keeps numerical features like Transaction Amount\n",
        ")\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (stratify to preserve fraud balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Create a pipeline with preprocessing + Random Forest classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Step 5: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "# Step 7: Output results\n",
        "print(\"\\n📊 Evaluation Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F2 Score:  {f2:.2f}\")\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Define features and target\n",
        "X = df.drop(columns=['Transaction ID', 'Fraudulent'])\n",
        "y = df['Fraudulent']\n",
        "\n",
        "# Step 2: Preprocessing (OneHotEncode categorical columns)\n",
        "categorical_features = ['Merchant ID', 'Transaction Type']\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keeps numerical features like Transaction Amount\n",
        ")\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (stratify to preserve fraud balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Create a pipeline with preprocessing + Random Forest classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Step 5: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "# Step 7: Output results\n",
        "print(\"\\n📊 Evaluation Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F2 Score:  {f2:.2f}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, classification_report\n",
        "\n",
        "# Step 1: Define features and target\n",
        "X = df.drop(columns=['Transaction ID', 'Fraudulent'])\n",
        "y = df['Fraudulent']\n",
        "\n",
        "# Step 2: Preprocessing (OneHotEncode categorical columns)\n",
        "categorical_features = ['Merchant ID', 'Transaction Type']\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keeps numerical features like Transaction Amount\n",
        ")\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (stratify to preserve fraud balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Create a pipeline with preprocessing + Random Forest classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Step 5: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "# Step 7: Output results\n",
        "print(\"\\n📊 Evaluation Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F2 Score:  {f2:.2f}\")\n",
        "\n",
        "print(\"\\n🧾 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"In evaluating metrics for this solution, we need consider that a false negative (missing an actual fraudulent transaction) means that fraud goes undetected, which could result in financial losses and damage to the institutional reputation. A false positive (flagging a legit transaction as fraud) could inconvenience users and lead to unnecessary investigations, but is less costly than missing actual fraud. Therefore we should proceed with the goal of maximizing recall (sensitivity) so that we catch as many fraudulent transactions as possible, while maintaining a reasonable level of precision to avoid too many false alarms.  To balance this, I’d select the F2 score as a main. Evaluation metric, as it gives more weight to recall than precision, and aligns with the requirement to minimize FN while still considering FP.  Based on using a random forest classifier, chosen because it works well for imbalanced classification problems like fraud detection, handles both numerical and categorical data, is less sensitive to noise and overfitting compared to a single decision tree, and ranks feature importance, helping with interpretability.   Findings: based on results, the model achieves a moderate recall (.330, meaning it correctly identifies a fair number of fraud cases. Precision is lower, due to the class imbalance and nature of fraud detection. The F2 of .31 provides a holistic view of performance that prioritizes fraud detection coverage. \")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}